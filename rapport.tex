\documentclass{article}

\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx} %package to manage images
\graphicspath{ {./images/} }

%%%%%%%%%%%%%%%% Lengths %%%%%%%%%%%%%%%%
\setlength{\textwidth}{15.5cm}
\setlength{\evensidemargin}{0.5cm}
\setlength{\oddsidemargin}{0.5cm}

%%%%%%%%%%%%%%%% Variables %%%%%%%%%%%%%%%%
\def\projet{2}
\def\titre{Méthode du gradient conjugué / Application à l’équation de la chaleur}
\def\groupe{4}
\def\equipe{12468}
\def\responsible{Enzo Médina}
\def\secretary{Perig Herau}
\def\others{Ghofrane Hamdouni, Hector Piteau}

\begin{document}

%%%%%%%%%%%%%%%% Header %%%%%%%%%%%%%%%%
\noindent\begin{minipage}{0.98\textwidth}
  \vskip 0mm
  \noindent
  { \begin{tabular}{p{7.5cm}}
      {\bfseries \sffamily
        Projet \projet} \\ 
      {\itshape \titre}
    \end{tabular}}
  \hfill 
  \fbox{\begin{tabular}{l}
      {~\hfill \bfseries \sffamily Groupe \groupe\ - Equipe \equipe
        \hfill~} \\[2mm] 
      Responsable : \responsible \\
      Secrétaire : \secretary \\
      Codeurs : \others
    \end{tabular}}
  \vskip 4mm ~

  ~~~\parbox{0.95\textwidth}{\small \textit{Résumé~: Dans ce projet nous cherchons à implémenter des algorithmes, direct et itératifs, de résolution de systèmes linéaires Ax = b avec A de grande taille. Nous allons nous intéresser tout particulièrement aux systèmes linéaires symétriques définis positifs et creux et discuter des efficacités et complexités de ces algorithmes.  } \sffamily  }
  \vskip 1mm ~
\end{minipage}

%%%%%%%%%%%%%%%% Main part %%%%%%%%%%%%%%%%
\section*{Présentation du travail réalisé}
    Dans la suite de ce rapport, nous allons vous présenter le travail que nous avons réalisé au cours du projet. Puis nous conclurons avec un retour sur le gain d'expérience que nous a apporté ce projet. 
\subsection*{Décomposition de Cholesky} 
    Cette partie a pour objectif d'écrire une méthode de factorisation en utilisant la décomposition de Cholesky classique, puis incomplète. \\
    
    1) Dans un premier temps, on s'occupe de la décomposition classique.
    Elle est défini par la formule suivante : 
    \begin{align}
        t_{i,i}^2 &= a_{i,i} - \sum_{k=1}^{i-1} t_{i,k}^2 \\
        t_{i,i}^2 &= \frac{a_{i,i} - \sum_{k=1}^{i-1} {t_{i,k}*t_{j,k}}}{t_{i,i}} \quad j \ge i\quad
    \end{align}
    
    Étant donné qu'on parcourt chaque élément de la matrice ($O(n^{2})$) et que les éléments diagonaux nécessitent d'accéder à l'ensemble des éléments de la ligne ($O(n)$), l'algorithme possède une complexité cubique $O(n^{3})$. \\
    
    2) (à remplir)


    3) On prend 2 paramètres pour créer la matrice : \\
\begin{itemize}
    \item N : Ce paramètre nous permet de connaître le nombre d'éléments extra diagonaux non nuls souhaités. Étant donné que la matrice retournée est symétrique, on considère uniquement le côté supérieur (ou inférieur) de la matrice pour la valeur de N. Ainsi, on a $0 <= N <= n*(n-1)/2$ avec n la taille de la matrice. \\
    
    \item Size : Paramètre permettant de définir la taille souhaitée de la matrice retournée. On a besoin d'un seul paramètre car on retourne une matrice carrée. Très utile pour les différents tests. \\
\end{itemize}
    
On souhaite retourner une matrice symétrique. Pour cela, on utilise le fait que le produit d'une matrice et de sa transposée retourne une matrice symétrique ($A = B . ^tB$ implique A symétrique). \\

Pour s'assurer que la matrice est bien définie positive, on assigne aux éléments diagonaux des valeurs telles qu'elles soient supérieures à la somme des valeurs absolues d'une même ligne (c'est une condition suffisante). Les termes extra diagonaux sont eux assignés aléatoirement par la fonction \textit{randint}. \\

Enfin, il s'agit de gérer le nombre de termes extra diagonaux non nuls, le paramètre N. On l'utilise pour déterminer le nombre de 0 dans la matrice. On assigne alors à la matrice ces valeurs nulles aux positions (i,j) mais également aux positions (j, i) pour conserver la symétrie de la matrice. \\
    
  4) La factorisation incomplète de Cholesky est très similaire à la factorisation classique. Il suffit simplement de vérifier si on a des éléments nuls dans la matrice d'origine, auquel cas on retourne 0 à la même position pour la matrice générée. \\
  
  Nous avons essayé plusieurs tests de performances :
  \begin{enumerate}
      \item Une génération de 1000 matrices de petites tailles (3 et 4). \\
      Les temps d'exécution des factorisations classiques et incomplètes sont très similaires (au centième de seconde près).
      Dans le cas de matrices contenant uniquement des 0 dans les éléments diagonaux, on remarque que la factorisation incomplète est à peine meilleur. A l'inverse, dans le cas de matrices sans termes nuls, c'est la factorisation classique qui est un tout petit peu plus performante ce qui est logique car on rajoute un test dans la version incomplète. \\
      
      \item Une unique matrice de très grande taille (700 x 700) \\
      Avec une matrice entièrement rempli d'éléments extra diagonaux nuls, le résultat est sans appel : \textbf{27s de temps d'exécution par la factorisation classique contre 0.2s pour la factorisation incomplète}.
      En revanche, pour des matrices ne contenant pas de 0, elles restent équivalentes.
  \end{enumerate}
  
  On comprends alors tout l'intérêt de chercher à réduire les matrices pour obtenir un maximum d'éléments extra diagonaux nuls. Les calculs sont bien plus rapide (Dans le cas de notre matrice 700 * 700, on parle d'une méthode 100 fois meilleure). \\
  
5) On définit une fonction pour obtenir le conditionnement du produit \(^tT^{-1} . T^{-1}\) et le conditionnement classique de A. Les résultats sont très proches (précision \(10^{-8}\)). Cependant, on remarque que pour la moitié des tests, le conditionnement calculé est supérieur au conditionnement de A. C'est donc un mauvais préconditionneur. L'autre moitié du temps, il est donc inférieur et correspond à un bon préconditionneur.


    
\subsection*{Méthode du gradient conjugué}
\paragraph{}
    Dans cette partie nous avons implémenté l'algorithme itératif de la méthode du gradient conjugué, dont le pseudo-algorithme est donné sur la page Wikipedia de l'algorithme \cite{gradconjugwiki}.

\paragraph{}
    Tout d'abord, il nous a été donné dans le sujet une implémentation Matlab dans laquelle nous avons relevé certains non respect d'un standard de programmation sain. Parmi ces derniers, nous avons relevé que les noms des variables ne sont pas toujours explicites, ce qui freine la compréhension de l'algorithme. De plus, le seuil de précision constituant la condition d'arrêt est inscrit en dur dans l'algorithme. Il est donc impossible de le changer au cours de l'exécution.
    
\paragraph{}
    Cet algorithme est cependant différent de la méthode de Cholesky décrite précédemment, car il ne nécessite pas d'utiliser un algorithme comme celui de Gauss (seules les remontées sont à effectuer). Ainsi, on obtient une complexité quadratique au lieu de cubique la plupart du temps. Cependant, dans le cas de matrices spécifiques, le gain de complexité est négligeable puisque la décomposition de Cholesky trouve directement la bonne forme de matrice pour simplifier la suite.

\paragraph{}
    La première méthode est est une implémentation directe. La présence de multiples produits matriciels de complexités respectivement linéaire et quadratiques, dans une boucle qui s'arrête à une précision demandée, donnent à l'algorithme une complexité en $O(log(i)*n^2)$ avec i la précision souhaitée. Deux paramètres ont été ajoutés, pour avoir un contrôle manuel sur la précision et le nombre maximum d'itérations directement en entrée de l'algorithme. 
    
\paragraph{}
    La seconde méthode utilise un préconditionneur sur la matrice d'entrée, dans le but de réduire le nombre d'itérations de l'algorithme. Cette méthode est de complexité similaire, mais produit moins d'itérations (bien que temporellement, elle soit plus coûteuse à l'exécution à cause du nombre de transformations), comme on peut le voir dans la figure \ref{fig:pconj_precond}. 
    
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\columnwidth]{TODO.png}
    \caption{...}
    \label{fig:conj_precond}
\end{figure}

\paragraph{}
    Pour éviter l'utilisation d'inversions de matrices (souvent imprécises et propices à nombre d'erreurs), une double remontée gaussienne a été utilisée, en utilisant la décomposition du préconditionneur en un produit de deux matrices triangulaires symétriques. Ainsi, une fonction pour automatiser ces remontées (sur une matrice triangulaire supérieure ou inférieure) a été définie.
    
    
\subsection*{Application à l’équation de la chaleur}
    Cette partie a pour but de mettre en oeuvre les deux précédente parties afin de résoudre une équation aux dérivées partielles, l'équation de la chaleur. Le but étant de simuler l'évolution de la température sur une grille de points.
    

%%%%%%%%%%%%%%%% End part %%%%%%%%%%%%%%%%
\section*{Conclusion et apports du projet}
\paragraph{}
    Les deux méthodes et leur amélioration respective (décomposition de Cholesky incomplète et gradient conjugué avec préconditionneur) sont viables dans le cas de la résolution d'un système linéaire en un temps raisonnable. Cependant, la seconde méthode semble faire ses preuves pour un apport non négligeable en complexité en nombre d'itérations, et ainsi peut être appliquée à des simulations comme celle de la troisième partie du projet.

\begin{thebibliography}{9}
\bibitem{gradconjugwiki} 
Algorithme de la décomposition de Cholesky sur Wikipedia.
\\\texttt{https://en.wikipedia.org/wiki/Conjugate\_gradient\_method}
\end{thebibliography}


\end{document}
